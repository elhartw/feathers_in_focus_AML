{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14010757,"sourceType":"datasetVersion","datasetId":8924522}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# **Data preparation**\n\nCreating a dataset that is compatible with torch.DataLoader\n","metadata":{}},{"cell_type":"code","source":"# importing packages\n\nimport numpy as np\nimport pandas as pd\nimport torch as tc\n\nfrom pathlib import Path\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import v2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:58:11.926138Z","iopub.execute_input":"2025-12-05T14:58:11.926562Z","iopub.status.idle":"2025-12-05T14:58:11.932079Z","shell.execute_reply.started":"2025-12-05T14:58:11.926530Z","shell.execute_reply":"2025-12-05T14:58:11.930943Z"}},"outputs":[],"execution_count":136},{"cell_type":"markdown","source":"Transforming images to make them compatible for CNN","metadata":{}},{"cell_type":"code","source":"TRANSFORM_DEFAULT = v2.Compose([\n    v2.ToImage(),\n    v2.Resize((224, 224)),\n    v2.ToDtype(tc.float32, scale=True), \n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:58:11.933629Z","iopub.execute_input":"2025-12-05T14:58:11.934063Z","iopub.status.idle":"2025-12-05T14:58:11.955012Z","shell.execute_reply.started":"2025-12-05T14:58:11.934017Z","shell.execute_reply":"2025-12-05T14:58:11.953920Z"}},"outputs":[],"execution_count":137},{"cell_type":"code","source":"DATA_DIR  = Path(\"/kaggle/input/feathers-in-focus-model/aml-2025-feathers-in-focus\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:58:11.956793Z","iopub.execute_input":"2025-12-05T14:58:11.957245Z","iopub.status.idle":"2025-12-05T14:58:11.974079Z","shell.execute_reply.started":"2025-12-05T14:58:11.957207Z","shell.execute_reply":"2025-12-05T14:58:11.973069Z"}},"outputs":[],"execution_count":138},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/feathers-in-focus-model//aml-2025-feathers-in-focus/train_images.csv\")\nprint(train_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:58:11.975168Z","iopub.execute_input":"2025-12-05T14:58:11.975711Z","iopub.status.idle":"2025-12-05T14:58:12.006489Z","shell.execute_reply.started":"2025-12-05T14:58:11.975665Z","shell.execute_reply":"2025-12-05T14:58:12.005376Z"}},"outputs":[{"name":"stdout","text":"            image_path  label\n0  /train_images/1.jpg      1\n1  /train_images/2.jpg      1\n2  /train_images/3.jpg      1\n3  /train_images/4.jpg      1\n4  /train_images/5.jpg      1\n","output_type":"stream"}],"execution_count":139},{"cell_type":"markdown","source":"Defining the type and config parameters of the data","metadata":{}},{"cell_type":"code","source":"# idx, image, label, path\n\nItemType = tuple[int, tc.Tensor, int, str]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:58:12.008764Z","iopub.execute_input":"2025-12-05T14:58:12.009128Z","iopub.status.idle":"2025-12-05T14:58:12.015080Z","shell.execute_reply.started":"2025-12-05T14:58:12.009100Z","shell.execute_reply":"2025-12-05T14:58:12.013770Z"}},"outputs":[],"execution_count":140},{"cell_type":"markdown","source":"Define how to read a sample given the index","metadata":{}},{"cell_type":"code","source":"class ImageClassification(Dataset[ItemType]):\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        search_root: Path,\n        transform: v2.Transform = TRANSFORM_DEFAULT,\n    ):\n        self.df = df.reset_index(drop=True)\n        self.search_root = search_root\n        self.transform = transform\n    \n    def __len__(self) -> int:\n        return len(self.df)\n    \n    def _find_image_path(self, filename: str) -> Path:\n        path = self.search_root / filename.lstrip('/')\n        if not path.exists():\n            raise FileNotFoundError(f\"Image file '{filename}' not found at {path}\")\n        return path\n    \n    def __getitem__(self, idx: int) -> ItemType:\n        row = self.df.iloc[idx]\n        \n        filename = Path(str(row[\"image_path\"])).name\n        \n        path = self._find_image_path(filename)\n        \n        image = Image.open(path).convert(\"RGB\")\n        image = self.transform(image)\n        \n        label = int(row[\"label\"]) - 1\n        \n        return idx, image, label, str(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:58:12.016211Z","iopub.execute_input":"2025-12-05T14:58:12.016570Z","iopub.status.idle":"2025-12-05T14:58:12.035320Z","shell.execute_reply.started":"2025-12-05T14:58:12.016533Z","shell.execute_reply":"2025-12-05T14:58:12.034032Z"}},"outputs":[],"execution_count":141},{"cell_type":"markdown","source":"Check if set-up works","metadata":{}},{"cell_type":"code","source":"SEARCH_ROOT = Path(\"/kaggle/input/feathers-in-focus-model/aml-2025-feathers-in-focus/train_images\")\n\ntrain_dataset = ImageClassification(\n    df=train_df,\n    search_root=SEARCH_ROOT,\n    transform=TRANSFORM_DEFAULT,\n)\n\nidx0, img0, label0, path0 = train_dataset[0]\nprint(\"Pad eerste image:\", path0)\nprint(\"Image shape:\", img0.shape)\nprint(\"Label:\", label0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:58:12.036397Z","iopub.execute_input":"2025-12-05T14:58:12.036718Z","iopub.status.idle":"2025-12-05T14:58:12.071181Z","shell.execute_reply.started":"2025-12-05T14:58:12.036693Z","shell.execute_reply":"2025-12-05T14:58:12.070040Z"}},"outputs":[{"name":"stdout","text":"Pad eerste image: /kaggle/input/feathers-in-focus-model/aml-2025-feathers-in-focus/train_images/1.jpg\nImage shape: torch.Size([3, 224, 224])\nLabel: 0\n","output_type":"stream"}],"execution_count":142},{"cell_type":"markdown","source":"Splitting the data in training and validation set (80/20) and creating DataLoader batches","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# Train/validation split\nfrom torch.utils.data import random_split\n\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_set, val_set = random_split(train_dataset, [train_size, val_size])\n\n# DataLoaders batches\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:58:12.072488Z","iopub.execute_input":"2025-12-05T14:58:12.072772Z","iopub.status.idle":"2025-12-05T14:58:12.080361Z","shell.execute_reply.started":"2025-12-05T14:58:12.072748Z","shell.execute_reply":"2025-12-05T14:58:12.079305Z"}},"outputs":[],"execution_count":143},{"cell_type":"markdown","source":"# **CREATING, TRAINING, VALIDATING MODEL**","metadata":{}},{"cell_type":"code","source":"import argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.optim.lr_scheduler import StepLR\n\nfrom pathlib import Path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:58:12.081543Z","iopub.execute_input":"2025-12-05T14:58:12.081849Z","iopub.status.idle":"2025-12-05T14:58:12.099822Z","shell.execute_reply.started":"2025-12-05T14:58:12.081822Z","shell.execute_reply":"2025-12-05T14:58:12.098698Z"}},"outputs":[],"execution_count":144},{"cell_type":"code","source":"DATA_DIR  = Path(\"/kaggle/input/feathers-in-focus-model/aml-2025-feathers-in-focus\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:58:12.102445Z","iopub.execute_input":"2025-12-05T14:58:12.102845Z","iopub.status.idle":"2025-12-05T14:58:12.130528Z","shell.execute_reply.started":"2025-12-05T14:58:12.102817Z","shell.execute_reply":"2025-12-05T14:58:12.129094Z"}},"outputs":[],"execution_count":145},{"cell_type":"code","source":"def load_argparser():\n    parser = argparse.ArgumentParser(description=\"Feathers in Focus\")\n    parser.add.argument(\"--batch-size\", type=int, default=64)\n    parser.add.argument(\"__test-batch-size\", type=int, default=1000)\n    parser.add.argument(\"--epochs\", type=int, default=14)\n    parser.add.argument(\"--lr\", type=float, default=1.0)\n    parder.add.argument(\"--gamma\", type=float, default=0.7)\n    parser.add.argument(\"--dry-run\", action=\"store_true\")\n    parser.add.argument(\"--seed\", type=int, default=1)\n    parser.add.argument(\"--log-interval\", type=int, default=10)\n    parder.add.argument(\"--save-model\", action=\"store_true\")\n    return parser","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:58:12.131986Z","iopub.execute_input":"2025-12-05T14:58:12.132382Z","iopub.status.idle":"2025-12-05T14:58:12.153112Z","shell.execute_reply.started":"2025-12-05T14:58:12.132354Z","shell.execute_reply":"2025-12-05T14:58:12.151746Z"}},"outputs":[],"execution_count":146},{"cell_type":"markdown","source":"**Defining the model to use**\n\nModel with 4 convolional layers (RGB, simple features, complex pattorns, higher level features). \nThis leads to a fully convolutional layer with 9216 input and 128 output. \nThis leads to 200 classes, bird species. ","metadata":{}},{"cell_type":"markdown","source":"**Forward pass**\n\nForward pass trough the model, using the 4 convolutional layers, combined with the ReLu activation function for non-linearity. Making use of max pooling and creating the fully connected layer followed by the output layer and softmax function. ","metadata":{}},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n        self.conv4 = nn.Conv2d(64, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 200)\n    \n    def forward(self, x): \n        x = self.conv1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        \n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        \n        x = self.conv3(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        \n        x = self.conv4(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        \n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:58:12.154297Z","iopub.execute_input":"2025-12-05T14:58:12.154593Z","iopub.status.idle":"2025-12-05T14:58:12.173339Z","shell.execute_reply.started":"2025-12-05T14:58:12.154568Z","shell.execute_reply":"2025-12-05T14:58:12.172108Z"}},"outputs":[],"execution_count":147},{"cell_type":"markdown","source":"**Training loop: forward - backward propagation**\n\nFrom forward pass, to calculating the loss, to backward propagation calculating the gradient to adjusting the weights. ","metadata":{}},{"cell_type":"code","source":"def train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (idx, data, target, path) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        if batch_idx % 10 == 0:\n            print(f\"Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:58:12.174387Z","iopub.execute_input":"2025-12-05T14:58:12.174913Z","iopub.status.idle":"2025-12-05T14:58:12.191519Z","shell.execute_reply.started":"2025-12-05T14:58:12.174877Z","shell.execute_reply":"2025-12-05T14:58:12.190507Z"}},"outputs":[],"execution_count":148},{"cell_type":"markdown","source":"**Testing / validation loop**","metadata":{}},{"cell_type":"code","source":"def test(model, device, test_loader):\n    model.eval()      \n    test_loss = 0\n    correct = 0\n    \n    with torch.no_grad():  \n        for idx, data, target, path in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            \n            test_loss += F.nll_loss(output, target, reduction=\"sum\").item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n    \n    test_loss /= len(test_loader.dataset)\n    accuracy = 100. * correct / len(test_loader.dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:58:12.192577Z","iopub.execute_input":"2025-12-05T14:58:12.192928Z","iopub.status.idle":"2025-12-05T14:58:12.215899Z","shell.execute_reply.started":"2025-12-05T14:58:12.192896Z","shell.execute_reply":"2025-12-05T14:58:12.214918Z"}},"outputs":[],"execution_count":149},{"cell_type":"code","source":"# Config\ntorch.manual_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hyperparameters\nEPOCHS = 14\nLR = 1.0\nGAMMA = 0.7\n\n# Model, optimizer, scheduler\nmodel = Net().to(device)\noptimizer = optim.Adadelta(model.parameters(), lr=LR)\nscheduler = StepLR(optimizer, step_size=1, gamma=GAMMA)\n\n# Training loop\nfor epoch in range(1, EPOCHS + 1):\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, val_loader)\n    scheduler.step()\n\n# Model opslaan\ntorch.save(model.state_dict(), \"bird_cnn.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T14:58:12.217260Z","iopub.execute_input":"2025-12-05T14:58:12.217607Z","iopub.status.idle":"2025-12-05T16:11:46.589101Z","shell.execute_reply.started":"2025-12-05T14:58:12.217570Z","shell.execute_reply":"2025-12-05T16:11:46.587928Z"}},"outputs":[{"name":"stdout","text":"Epoch: 1 [0/3140] Loss: 5.297921\nEpoch: 1 [320/3140] Loss: 5.283964\nEpoch: 1 [640/3140] Loss: 5.305070\nEpoch: 1 [960/3140] Loss: 5.288612\nEpoch: 1 [1280/3140] Loss: 5.282690\nEpoch: 1 [1600/3140] Loss: 5.311795\nEpoch: 1 [1920/3140] Loss: 5.312905\nEpoch: 1 [2240/3140] Loss: 5.218908\nEpoch: 1 [2560/3140] Loss: 5.282810\nEpoch: 1 [2880/3140] Loss: 5.261921\nEpoch: 2 [0/3140] Loss: 5.323692\nEpoch: 2 [320/3140] Loss: 5.265994\nEpoch: 2 [640/3140] Loss: 5.258194\nEpoch: 2 [960/3140] Loss: 5.254118\nEpoch: 2 [1280/3140] Loss: 5.232336\nEpoch: 2 [1600/3140] Loss: 5.226178\nEpoch: 2 [1920/3140] Loss: 5.200723\nEpoch: 2 [2240/3140] Loss: 5.209565\nEpoch: 2 [2560/3140] Loss: 5.288154\nEpoch: 2 [2880/3140] Loss: 5.265965\nEpoch: 3 [0/3140] Loss: 5.230252\nEpoch: 3 [320/3140] Loss: 5.282146\nEpoch: 3 [640/3140] Loss: 5.229700\nEpoch: 3 [960/3140] Loss: 5.192050\nEpoch: 3 [1280/3140] Loss: 5.220531\nEpoch: 3 [1600/3140] Loss: 5.055368\nEpoch: 3 [1920/3140] Loss: 5.349514\nEpoch: 3 [2240/3140] Loss: 5.268039\nEpoch: 3 [2560/3140] Loss: 5.130763\nEpoch: 3 [2880/3140] Loss: 5.300405\nEpoch: 4 [0/3140] Loss: 5.171601\nEpoch: 4 [320/3140] Loss: 5.206423\nEpoch: 4 [640/3140] Loss: 5.211146\nEpoch: 4 [960/3140] Loss: 5.154999\nEpoch: 4 [1280/3140] Loss: 5.124417\nEpoch: 4 [1600/3140] Loss: 5.296360\nEpoch: 4 [1920/3140] Loss: 5.278877\nEpoch: 4 [2240/3140] Loss: 5.181222\nEpoch: 4 [2560/3140] Loss: 5.106931\nEpoch: 4 [2880/3140] Loss: 5.282750\nEpoch: 5 [0/3140] Loss: 5.124370\nEpoch: 5 [320/3140] Loss: 5.196111\nEpoch: 5 [640/3140] Loss: 5.205639\nEpoch: 5 [960/3140] Loss: 5.012646\nEpoch: 5 [1280/3140] Loss: 5.195405\nEpoch: 5 [1600/3140] Loss: 5.243909\nEpoch: 5 [1920/3140] Loss: 5.188636\nEpoch: 5 [2240/3140] Loss: 5.142602\nEpoch: 5 [2560/3140] Loss: 5.267909\nEpoch: 5 [2880/3140] Loss: 5.165672\nEpoch: 6 [0/3140] Loss: 4.983078\nEpoch: 6 [320/3140] Loss: 5.178096\nEpoch: 6 [640/3140] Loss: 4.974096\nEpoch: 6 [960/3140] Loss: 5.183352\nEpoch: 6 [1280/3140] Loss: 5.138910\nEpoch: 6 [1600/3140] Loss: 5.095225\nEpoch: 6 [1920/3140] Loss: 5.309523\nEpoch: 6 [2240/3140] Loss: 5.129997\nEpoch: 6 [2560/3140] Loss: 5.042870\nEpoch: 6 [2880/3140] Loss: 5.013503\nEpoch: 7 [0/3140] Loss: 5.154830\nEpoch: 7 [320/3140] Loss: 4.855658\nEpoch: 7 [640/3140] Loss: 4.992226\nEpoch: 7 [960/3140] Loss: 5.204055\nEpoch: 7 [1280/3140] Loss: 5.054415\nEpoch: 7 [1600/3140] Loss: 5.329918\nEpoch: 7 [1920/3140] Loss: 4.862874\nEpoch: 7 [2240/3140] Loss: 5.030511\nEpoch: 7 [2560/3140] Loss: 5.091558\nEpoch: 7 [2880/3140] Loss: 5.081921\nEpoch: 8 [0/3140] Loss: 5.095353\nEpoch: 8 [320/3140] Loss: 4.946497\nEpoch: 8 [640/3140] Loss: 4.873725\nEpoch: 8 [960/3140] Loss: 4.969944\nEpoch: 8 [1280/3140] Loss: 4.950000\nEpoch: 8 [1600/3140] Loss: 5.270922\nEpoch: 8 [1920/3140] Loss: 5.103181\nEpoch: 8 [2240/3140] Loss: 4.867850\nEpoch: 8 [2560/3140] Loss: 5.123319\nEpoch: 8 [2880/3140] Loss: 4.903153\nEpoch: 9 [0/3140] Loss: 5.011927\nEpoch: 9 [320/3140] Loss: 5.046885\nEpoch: 9 [640/3140] Loss: 4.979729\nEpoch: 9 [960/3140] Loss: 5.169592\nEpoch: 9 [1280/3140] Loss: 4.835060\nEpoch: 9 [1600/3140] Loss: 4.908692\nEpoch: 9 [1920/3140] Loss: 5.260389\nEpoch: 9 [2240/3140] Loss: 4.955725\nEpoch: 9 [2560/3140] Loss: 4.936803\nEpoch: 9 [2880/3140] Loss: 4.765285\nEpoch: 10 [0/3140] Loss: 4.981730\nEpoch: 10 [320/3140] Loss: 5.037529\nEpoch: 10 [640/3140] Loss: 5.052388\nEpoch: 10 [960/3140] Loss: 5.001321\nEpoch: 10 [1280/3140] Loss: 4.967685\nEpoch: 10 [1600/3140] Loss: 5.175933\nEpoch: 10 [1920/3140] Loss: 5.127057\nEpoch: 10 [2240/3140] Loss: 5.024052\nEpoch: 10 [2560/3140] Loss: 4.887148\nEpoch: 10 [2880/3140] Loss: 4.933271\nEpoch: 11 [0/3140] Loss: 4.920538\nEpoch: 11 [320/3140] Loss: 5.045185\nEpoch: 11 [640/3140] Loss: 5.005373\nEpoch: 11 [960/3140] Loss: 5.082018\nEpoch: 11 [1280/3140] Loss: 5.029845\nEpoch: 11 [1600/3140] Loss: 5.014489\nEpoch: 11 [1920/3140] Loss: 4.713578\nEpoch: 11 [2240/3140] Loss: 5.146379\nEpoch: 11 [2560/3140] Loss: 4.977230\nEpoch: 11 [2880/3140] Loss: 4.855903\nEpoch: 12 [0/3140] Loss: 4.836991\nEpoch: 12 [320/3140] Loss: 4.771709\nEpoch: 12 [640/3140] Loss: 4.940699\nEpoch: 12 [960/3140] Loss: 4.906769\nEpoch: 12 [1280/3140] Loss: 5.132894\nEpoch: 12 [1600/3140] Loss: 5.057154\nEpoch: 12 [1920/3140] Loss: 4.948934\nEpoch: 12 [2240/3140] Loss: 5.050469\nEpoch: 12 [2560/3140] Loss: 4.786510\nEpoch: 12 [2880/3140] Loss: 4.921214\nEpoch: 13 [0/3140] Loss: 4.919309\nEpoch: 13 [320/3140] Loss: 4.876040\nEpoch: 13 [640/3140] Loss: 5.070277\nEpoch: 13 [960/3140] Loss: 5.047245\nEpoch: 13 [1280/3140] Loss: 4.961961\nEpoch: 13 [1600/3140] Loss: 4.837021\nEpoch: 13 [1920/3140] Loss: 5.250453\nEpoch: 13 [2240/3140] Loss: 4.849531\nEpoch: 13 [2560/3140] Loss: 5.001692\nEpoch: 13 [2880/3140] Loss: 5.155438\nEpoch: 14 [0/3140] Loss: 4.692647\nEpoch: 14 [320/3140] Loss: 4.937731\nEpoch: 14 [640/3140] Loss: 5.109867\nEpoch: 14 [960/3140] Loss: 4.865740\nEpoch: 14 [1280/3140] Loss: 4.609324\nEpoch: 14 [1600/3140] Loss: 4.905317\nEpoch: 14 [1920/3140] Loss: 5.013296\nEpoch: 14 [2240/3140] Loss: 4.735778\nEpoch: 14 [2560/3140] Loss: 4.600466\nEpoch: 14 [2880/3140] Loss: 5.258226\n","output_type":"stream"}],"execution_count":150},{"cell_type":"markdown","source":"**Creating predictions on the test set and creating a file for submission in Kaggle**","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/feathers-in-focus-model/aml-2025-feathers-in-focus/test_images_path.csv\")\ntest_dataset = ImageClassification(\n    df=test_df,\n    search_root=Path(\"/kaggle/input/feathers-in-focus-model/aml-2025-feathers-in-focus/test_images\"),\n    transform=TRANSFORM_DEFAULT\n)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nmodel.eval()\npredictions = []\n\nwith torch.no_grad():\n    for idx, data, _, path in test_loader:\n        data = data.to(device)\n        output = model(data)\n        pred = output.argmax(dim=1) + 1  # +1 want Kaggle verwacht 1-200\n        predictions.extend(zip(idx.tolist(), pred.tolist()))\n\nsubmission = pd.DataFrame(predictions, columns=[\"id\", \"label\"])\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T16:14:51.052834Z","iopub.execute_input":"2025-12-05T16:14:51.053250Z","iopub.status.idle":"2025-12-05T16:18:21.134519Z","shell.execute_reply.started":"2025-12-05T16:14:51.053219Z","shell.execute_reply":"2025-12-05T16:18:21.133054Z"}},"outputs":[{"name":"stdout","text":"   id  label\n0   0     46\n1   1     47\n2   2      1\n3   3     10\n4   4     19\n","output_type":"stream"}],"execution_count":154},{"cell_type":"code","source":"submission[\"id\"] = test_df[\"id\"].values\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-05T16:25:37.888248Z","iopub.execute_input":"2025-12-05T16:25:37.888646Z","iopub.status.idle":"2025-12-05T16:25:37.904795Z","shell.execute_reply.started":"2025-12-05T16:25:37.888617Z","shell.execute_reply":"2025-12-05T16:25:37.903519Z"}},"outputs":[{"name":"stdout","text":"   id  label\n0   1     46\n1   2     47\n2   3      1\n3   4     10\n4   5     19\n","output_type":"stream"}],"execution_count":157}]}