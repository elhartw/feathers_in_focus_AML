{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14010757,"sourceType":"datasetVersion","datasetId":8924522}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# IMPORTS\n\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom pathlib import Path\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import v2\nfrom torch.optim.lr_scheduler import StepLR\n\n\n# DATA SETUP\n\n\nDATA_DIR = Path(\"/kaggle/input/feathers-in-focus-model/aml-2025-feathers-in-focus\")\n\nTRANSFORM_TRAIN = v2.Compose([\n    v2.ToImage(),\n    v2.Resize((224, 224)),\n    v2.RandomHorizontalFlip(p=0.5),\n    v2.RandomRotation(degrees=10),\n    v2.ColorJitter(brightness=0.2, contrast=0.2),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nTRANSFORM_VAL = v2.Compose([\n    v2.ToImage(),\n    v2.Resize((224, 224)),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Laad attributen\nattributes = np.load(DATA_DIR / \"attributes.npy\")\nattributes = torch.tensor(attributes, dtype=torch.float32)\nprint(f\"Attributes shape: {attributes.shape}\")\n\n\n# DATASET\n\n\nclass BirdDataset(Dataset):\n    def __init__(self, df, search_root, transform, attributes, test_mode=False):\n        self.df = df.reset_index(drop=True)\n        self.search_root = search_root\n        self.transform = transform\n        self.attributes = attributes\n        self.test_mode = test_mode\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        filename = Path(str(row[\"image_path\"])).name\n        path = self.search_root / filename.lstrip('/')\n        image = Image.open(path).convert(\"RGB\")\n        image = self.transform(image)\n        \n        if self.test_mode:\n            return image\n        \n        label = int(row[\"label\"]) - 1\n        attrs = self.attributes[label]\n        return image, label, attrs\n\n\n# DATA LOADERS\n\n\ntrain_df = pd.read_csv(DATA_DIR / \"train_images.csv\")\n\ntorch.manual_seed(42)\nindices = torch.randperm(len(train_df)).tolist()\ntrain_size = int(0.8 * len(train_df))\n\ntrain_dataset = BirdDataset(\n    df=train_df.iloc[indices[:train_size]].reset_index(drop=True),\n    search_root=DATA_DIR / \"train_images\",\n    transform=TRANSFORM_TRAIN,\n    attributes=attributes,\n)\nval_dataset = BirdDataset(\n    df=train_df.iloc[indices[train_size:]].reset_index(drop=True),\n    search_root=DATA_DIR / \"train_images\",\n    transform=TRANSFORM_VAL,\n    attributes=attributes,\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n\n# MODEL - Verbeterd met BatchNorm + extra layer\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        # Conv block 1: 3 -> 32\n        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n        self.bn1 = nn.BatchNorm2d(32)\n        \n        # Conv block 2: 32 -> 64\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.bn2 = nn.BatchNorm2d(64)\n        \n        # Conv block 3: 64 -> 128\n        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n        self.bn3 = nn.BatchNorm2d(128)\n        \n        # Conv block 4: 128 -> 128\n        self.conv4 = nn.Conv2d(128, 128, 3, 1)\n        self.bn4 = nn.BatchNorm2d(128)\n        \n        # Conv block 5: 128 -> 128 (NIEUW)\n        self.conv5 = nn.Conv2d(128, 128, 3, 1)\n        self.bn5 = nn.BatchNorm2d(128)\n        \n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        \n        # FC layers (input: 128 * 5 * 5 = 3200)\n        self.fc1 = nn.Linear(3200, 256)\n        self.fc2 = nn.Linear(256, 200)\n        self.fc_attr = nn.Linear(256, 312)\n    \n    def forward(self, x):\n        # Block 1: 224 -> 222 -> 111\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        \n        # Block 2: 111 -> 109 -> 54\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        \n        # Block 3: 54 -> 52 -> 26\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        \n        # Block 4: 26 -> 24 -> 12\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        \n        # Block 5: 12 -> 10 -> 5 (NIEUW)\n        x = self.conv5(x)\n        x = self.bn5(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        \n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        features = self.dropout2(x)\n        \n        # Classification output\n        class_out = self.fc2(features)\n        class_out = F.log_softmax(class_out, dim=1)\n        \n        # Attribute output\n        attr_out = torch.sigmoid(self.fc_attr(features))\n        \n        return class_out, attr_out\n\n\n# TRAINING\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\nmodel = Net().to(device)\nprint(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n\n\nEPOCHS = 40\nATTR_WEIGHT = 0.5\nbest_acc = 0.0\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    total_loss = 0\n    correct = 0\n    \n    for images, labels, target_attrs in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        target_attrs = target_attrs.to(device)\n        \n        optimizer.zero_grad()\n        class_out, attr_out = model(images)\n        \n        loss_class = F.nll_loss(class_out, labels)\n        loss_attr = F.binary_cross_entropy(attr_out, target_attrs)\n        loss = loss_class + ATTR_WEIGHT * loss_attr\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        correct += (class_out.argmax(1) == labels).sum().item()\n    \n    train_acc = correct / len(train_loader.dataset)\n    \n    model.eval()\n    val_correct = 0\n    with torch.no_grad():\n        for images, labels, _ in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            class_out, _ = model(images)\n            val_correct += (class_out.argmax(1) == labels).sum().item()\n    \n    val_acc = val_correct / len(val_loader.dataset)\n    \n    print(f\"Epoch {epoch}/{EPOCHS} | Loss: {total_loss:.1f} | Train: {train_acc:.4f} | Val: {val_acc:.4f}\")\n    \n    if val_acc > best_acc:\n        best_acc = val_acc\n        torch.save(model.state_dict(), \"bird_cnn.pt\")\n        print(f\"  ↳ Saved!\")\n    \n    scheduler.step(val_acc) \n\nprint(f\"\\nBest: {best_acc:.4f}\")\n\n\n\n# SUBMISSION MET TTA\n\n\nmodel.load_state_dict(torch.load(\"bird_cnn.pt\"))\nmodel.eval()\n\ntest_df = pd.read_csv(DATA_DIR / \"test_images_path.csv\")\n\nTTA_TRANSFORMS = [\n    v2.Compose([\n        v2.ToImage(),\n        v2.Resize((224, 224)),\n        v2.ToDtype(torch.float32, scale=True),\n        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]),\n    v2.Compose([\n        v2.ToImage(),\n        v2.Resize((224, 224)),\n        v2.RandomHorizontalFlip(p=1.0),\n        v2.ToDtype(torch.float32, scale=True),\n        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]),\n]\n\npredictions = []\nwith torch.no_grad():\n    for idx in range(len(test_df)):\n        row = test_df.iloc[idx]\n        filename = Path(str(row[\"image_path\"])).name\n        path = DATA_DIR / \"test_images\" / filename.lstrip('/')\n        image = Image.open(path).convert(\"RGB\")\n        \n        all_outputs = []\n        for transform in TTA_TRANSFORMS:\n            img_t = transform(image).unsqueeze(0).to(device)\n            output, _ = model(img_t)\n            all_outputs.append(output)\n        \n        avg_output = torch.stack(all_outputs).mean(0)\n        pred = avg_output.argmax(1).item() + 1\n        predictions.append(pred)\n\nsubmission = pd.DataFrame({\"id\": test_df[\"id\"].values, \"label\": predictions})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(submission.head())\nprint(\"Done!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-12T18:00:20.670057Z","iopub.execute_input":"2025-12-12T18:00:20.670374Z","iopub.status.idle":"2025-12-12T18:30:24.982977Z","shell.execute_reply.started":"2025-12-12T18:00:20.670324Z","shell.execute_reply":"2025-12-12T18:30:24.981967Z"}},"outputs":[{"name":"stdout","text":"Attributes shape: torch.Size([200, 312])\nDevice: cuda\nParameters: 1,340,416\nEpoch 1/40 | Loss: 548.6 | Train: 0.0099 | Val: 0.0051\n  ↳ Saved!\nEpoch 2/40 | Loss: 522.9 | Train: 0.0102 | Val: 0.0216\n  ↳ Saved!\nEpoch 3/40 | Loss: 511.0 | Train: 0.0178 | Val: 0.0280\n  ↳ Saved!\nEpoch 4/40 | Loss: 501.5 | Train: 0.0226 | Val: 0.0318\n  ↳ Saved!\nEpoch 5/40 | Loss: 492.5 | Train: 0.0271 | Val: 0.0407\n  ↳ Saved!\nEpoch 6/40 | Loss: 482.7 | Train: 0.0318 | Val: 0.0445\n  ↳ Saved!\nEpoch 7/40 | Loss: 475.9 | Train: 0.0357 | Val: 0.0534\n  ↳ Saved!\nEpoch 8/40 | Loss: 470.5 | Train: 0.0411 | Val: 0.0547\n  ↳ Saved!\nEpoch 9/40 | Loss: 464.2 | Train: 0.0360 | Val: 0.0458\nEpoch 10/40 | Loss: 460.9 | Train: 0.0430 | Val: 0.0751\n  ↳ Saved!\nEpoch 11/40 | Loss: 453.8 | Train: 0.0538 | Val: 0.0534\nEpoch 12/40 | Loss: 453.0 | Train: 0.0538 | Val: 0.0662\nEpoch 13/40 | Loss: 449.6 | Train: 0.0573 | Val: 0.0725\nEpoch 14/40 | Loss: 440.6 | Train: 0.0567 | Val: 0.0852\n  ↳ Saved!\nEpoch 15/40 | Loss: 439.5 | Train: 0.0561 | Val: 0.0916\n  ↳ Saved!\nEpoch 16/40 | Loss: 433.3 | Train: 0.0650 | Val: 0.0776\nEpoch 17/40 | Loss: 431.9 | Train: 0.0621 | Val: 0.0611\nEpoch 18/40 | Loss: 429.1 | Train: 0.0567 | Val: 0.0903\nEpoch 19/40 | Loss: 425.1 | Train: 0.0720 | Val: 0.0980\n  ↳ Saved!\nEpoch 20/40 | Loss: 422.3 | Train: 0.0732 | Val: 0.0891\nEpoch 21/40 | Loss: 420.5 | Train: 0.0739 | Val: 0.1043\n  ↳ Saved!\nEpoch 22/40 | Loss: 414.7 | Train: 0.0771 | Val: 0.1056\n  ↳ Saved!\nEpoch 23/40 | Loss: 412.7 | Train: 0.0815 | Val: 0.0992\nEpoch 24/40 | Loss: 412.8 | Train: 0.0818 | Val: 0.1132\n  ↳ Saved!\nEpoch 25/40 | Loss: 407.8 | Train: 0.0815 | Val: 0.1018\nEpoch 26/40 | Loss: 406.1 | Train: 0.0828 | Val: 0.1221\n  ↳ Saved!\nEpoch 27/40 | Loss: 405.6 | Train: 0.0796 | Val: 0.1183\nEpoch 28/40 | Loss: 401.9 | Train: 0.0809 | Val: 0.0967\nEpoch 29/40 | Loss: 397.6 | Train: 0.0876 | Val: 0.1031\nEpoch 30/40 | Loss: 395.0 | Train: 0.0949 | Val: 0.1260\n  ↳ Saved!\nEpoch 31/40 | Loss: 393.8 | Train: 0.0939 | Val: 0.1183\nEpoch 32/40 | Loss: 391.3 | Train: 0.0892 | Val: 0.1247\nEpoch 33/40 | Loss: 392.3 | Train: 0.0952 | Val: 0.1260\nEpoch 34/40 | Loss: 386.4 | Train: 0.0939 | Val: 0.1196\nEpoch 35/40 | Loss: 390.1 | Train: 0.0959 | Val: 0.1260\nEpoch 36/40 | Loss: 387.5 | Train: 0.1041 | Val: 0.1285\n  ↳ Saved!\nEpoch 37/40 | Loss: 385.5 | Train: 0.1029 | Val: 0.1399\n  ↳ Saved!\nEpoch 38/40 | Loss: 380.1 | Train: 0.1054 | Val: 0.1247\nEpoch 39/40 | Loss: 381.1 | Train: 0.0981 | Val: 0.1285\nEpoch 40/40 | Loss: 375.4 | Train: 0.1115 | Val: 0.1158\n\nBest: 0.1399\n   id  label\n0   1     87\n1   2     43\n2   3     79\n3   4     12\n4   5     73\nDone!\n","output_type":"stream"}],"execution_count":21}]}