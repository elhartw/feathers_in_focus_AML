{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14010757,"sourceType":"datasetVersion","datasetId":8924522}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# IMPORTS\n\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom pathlib import Path\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import v2\nfrom torch.optim.lr_scheduler import StepLR\n\n\n# DATA SETUP\n\n\nDATA_DIR = Path(\"/kaggle/input/feathers-in-focus-model/aml-2025-feathers-in-focus\")\n\nTRANSFORM_TRAIN = v2.Compose([\n    v2.ToImage(),\n    v2.Resize((224, 224)),\n    v2.RandomHorizontalFlip(p=0.5),\n    v2.RandomRotation(degrees=10),\n    v2.ColorJitter(brightness=0.2, contrast=0.2),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nTRANSFORM_VAL = v2.Compose([\n    v2.ToImage(),\n    v2.Resize((224, 224)),\n    v2.ToDtype(torch.float32, scale=True),\n    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Laad attributen\nattributes = np.load(DATA_DIR / \"attributes.npy\")\nattributes = torch.tensor(attributes, dtype=torch.float32)\nprint(f\"Attributes shape: {attributes.shape}\")\n\n\n# DATASET\n\n\nclass BirdDataset(Dataset):\n    def __init__(self, df, search_root, transform, attributes, test_mode=False):\n        self.df = df.reset_index(drop=True)\n        self.search_root = search_root\n        self.transform = transform\n        self.attributes = attributes\n        self.test_mode = test_mode\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        filename = Path(str(row[\"image_path\"])).name\n        path = self.search_root / filename.lstrip('/')\n        image = Image.open(path).convert(\"RGB\")\n        image = self.transform(image)\n        \n        if self.test_mode:\n            return image\n        \n        label = int(row[\"label\"]) - 1\n        attrs = self.attributes[label]\n        return image, label, attrs\n\n\n# DATA LOADERS\n\n\ntrain_df = pd.read_csv(DATA_DIR / \"train_images.csv\")\n\ntorch.manual_seed(42)\nindices = torch.randperm(len(train_df)).tolist()\ntrain_size = int(0.8 * len(train_df))\n\ntrain_dataset = BirdDataset(\n    df=train_df.iloc[indices[:train_size]].reset_index(drop=True),\n    search_root=DATA_DIR / \"train_images\",\n    transform=TRANSFORM_TRAIN,\n    attributes=attributes,\n)\nval_dataset = BirdDataset(\n    df=train_df.iloc[indices[train_size:]].reset_index(drop=True),\n    search_root=DATA_DIR / \"train_images\",\n    transform=TRANSFORM_VAL,\n    attributes=attributes,\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n\n# MODEL - Verbeterd met BatchNorm + extra layer\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        # Conv block 1: 3 -> 32\n        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n        self.bn1 = nn.BatchNorm2d(32)\n        \n        # Conv block 2: 32 -> 64\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.bn2 = nn.BatchNorm2d(64)\n        \n        # Conv block 3: 64 -> 128\n        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n        self.bn3 = nn.BatchNorm2d(128)\n        \n        # Conv block 4: 128 -> 128\n        self.conv4 = nn.Conv2d(128, 128, 3, 1)\n        self.bn4 = nn.BatchNorm2d(128)\n        \n        # Conv block 5: 128 -> 128 (NIEUW)\n        self.conv5 = nn.Conv2d(128, 128, 3, 1)\n        self.bn5 = nn.BatchNorm2d(128)\n        \n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        \n        # FC layers (input: 128 * 5 * 5 = 3200)\n        self.fc1 = nn.Linear(3200, 256)\n        self.fc2 = nn.Linear(256, 200)\n        self.fc_attr = nn.Linear(256, 312)\n    \n    def forward(self, x):\n        # Block 1: 224 -> 222 -> 111\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        \n        # Block 2: 111 -> 109 -> 54\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        \n        # Block 3: 54 -> 52 -> 26\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        \n        # Block 4: 26 -> 24 -> 12\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        \n        # Block 5: 12 -> 10 -> 5 (NIEUW)\n        x = self.conv5(x)\n        x = self.bn5(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        \n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        features = self.dropout2(x)\n        \n        # Classification output\n        class_out = self.fc2(features)\n        class_out = F.log_softmax(class_out, dim=1)\n        \n        # Attribute output\n        attr_out = torch.sigmoid(self.fc_attr(features))\n        \n        return class_out, attr_out\n\n\n# TRAINING\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\nmodel = Net().to(device)\nprint(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n\n\nEPOCHS = 60\nATTR_WEIGHT = 0.5\nbest_acc = 0.0\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    total_loss = 0\n    correct = 0\n    \n    for images, labels, target_attrs in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        target_attrs = target_attrs.to(device)\n        \n        optimizer.zero_grad()\n        class_out, attr_out = model(images)\n        \n        loss_class = F.nll_loss(class_out, labels)\n        loss_attr = F.binary_cross_entropy(attr_out, target_attrs)\n        loss = loss_class + ATTR_WEIGHT * loss_attr\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        correct += (class_out.argmax(1) == labels).sum().item()\n    \n    train_acc = correct / len(train_loader.dataset)\n    \n    model.eval()\n    val_correct = 0\n    with torch.no_grad():\n        for images, labels, _ in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            class_out, _ = model(images)\n            val_correct += (class_out.argmax(1) == labels).sum().item()\n    \n    val_acc = val_correct / len(val_loader.dataset)\n    \n    print(f\"Epoch {epoch}/{EPOCHS} | Loss: {total_loss:.1f} | Train: {train_acc:.4f} | Val: {val_acc:.4f}\")\n    \n    if val_acc > best_acc:\n        best_acc = val_acc\n        torch.save(model.state_dict(), \"bird_cnn.pt\")\n        print(f\"  ↳ Saved!\")\n    \n    scheduler.step(val_acc) \n\nprint(f\"\\nBest: {best_acc:.4f}\")\n\n\n\n# SUBMISSION MET TTA\n\n\nmodel.load_state_dict(torch.load(\"bird_cnn.pt\"))\nmodel.eval()\n\ntest_df = pd.read_csv(DATA_DIR / \"test_images_path.csv\")\n\nTTA_TRANSFORMS = [\n    v2.Compose([\n        v2.ToImage(),\n        v2.Resize((224, 224)),\n        v2.ToDtype(torch.float32, scale=True),\n        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]),\n    v2.Compose([\n        v2.ToImage(),\n        v2.Resize((224, 224)),\n        v2.RandomHorizontalFlip(p=1.0),\n        v2.ToDtype(torch.float32, scale=True),\n        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]),\n]\n\npredictions = []\nwith torch.no_grad():\n    for idx in range(len(test_df)):\n        row = test_df.iloc[idx]\n        filename = Path(str(row[\"image_path\"])).name\n        path = DATA_DIR / \"test_images\" / filename.lstrip('/')\n        image = Image.open(path).convert(\"RGB\")\n        \n        all_outputs = []\n        for transform in TTA_TRANSFORMS:\n            img_t = transform(image).unsqueeze(0).to(device)\n            output, _ = model(img_t)\n            all_outputs.append(output)\n        \n        avg_output = torch.stack(all_outputs).mean(0)\n        pred = avg_output.argmax(1).item() + 1\n        predictions.append(pred)\n\nsubmission = pd.DataFrame({\"id\": test_df[\"id\"].values, \"label\": predictions})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(submission.head())\nprint(\"Done!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-12T18:41:28.849975Z","iopub.execute_input":"2025-12-12T18:41:28.850601Z","iopub.status.idle":"2025-12-12T19:26:06.952841Z","shell.execute_reply.started":"2025-12-12T18:41:28.850579Z","shell.execute_reply":"2025-12-12T19:26:06.951815Z"}},"outputs":[{"name":"stdout","text":"Attributes shape: torch.Size([200, 312])\nDevice: cuda\nParameters: 1,340,416\nEpoch 1/60 | Loss: 547.4 | Train: 0.0096 | Val: 0.0089\n  ↳ Saved!\nEpoch 2/60 | Loss: 524.6 | Train: 0.0121 | Val: 0.0153\n  ↳ Saved!\nEpoch 3/60 | Loss: 516.5 | Train: 0.0194 | Val: 0.0229\n  ↳ Saved!\nEpoch 4/60 | Loss: 508.0 | Train: 0.0236 | Val: 0.0229\nEpoch 5/60 | Loss: 497.3 | Train: 0.0296 | Val: 0.0305\n  ↳ Saved!\nEpoch 6/60 | Loss: 488.7 | Train: 0.0303 | Val: 0.0318\n  ↳ Saved!\nEpoch 7/60 | Loss: 480.8 | Train: 0.0315 | Val: 0.0483\n  ↳ Saved!\nEpoch 8/60 | Loss: 472.9 | Train: 0.0408 | Val: 0.0496\n  ↳ Saved!\nEpoch 9/60 | Loss: 469.8 | Train: 0.0404 | Val: 0.0547\n  ↳ Saved!\nEpoch 10/60 | Loss: 461.7 | Train: 0.0443 | Val: 0.0509\nEpoch 11/60 | Loss: 455.4 | Train: 0.0497 | Val: 0.0611\n  ↳ Saved!\nEpoch 12/60 | Loss: 455.0 | Train: 0.0541 | Val: 0.0712\n  ↳ Saved!\nEpoch 13/60 | Loss: 450.8 | Train: 0.0506 | Val: 0.0662\nEpoch 14/60 | Loss: 444.2 | Train: 0.0615 | Val: 0.0700\nEpoch 15/60 | Loss: 443.3 | Train: 0.0602 | Val: 0.0814\n  ↳ Saved!\nEpoch 16/60 | Loss: 434.5 | Train: 0.0659 | Val: 0.0751\nEpoch 17/60 | Loss: 433.0 | Train: 0.0627 | Val: 0.0598\nEpoch 18/60 | Loss: 429.5 | Train: 0.0659 | Val: 0.0903\n  ↳ Saved!\nEpoch 19/60 | Loss: 424.3 | Train: 0.0701 | Val: 0.0878\nEpoch 20/60 | Loss: 424.9 | Train: 0.0764 | Val: 0.0929\n  ↳ Saved!\nEpoch 21/60 | Loss: 421.1 | Train: 0.0697 | Val: 0.0929\nEpoch 22/60 | Loss: 413.6 | Train: 0.0876 | Val: 0.1018\n  ↳ Saved!\nEpoch 23/60 | Loss: 414.3 | Train: 0.0783 | Val: 0.1069\n  ↳ Saved!\nEpoch 24/60 | Loss: 414.2 | Train: 0.0812 | Val: 0.1094\n  ↳ Saved!\nEpoch 25/60 | Loss: 408.9 | Train: 0.0863 | Val: 0.1120\n  ↳ Saved!\nEpoch 26/60 | Loss: 406.4 | Train: 0.0838 | Val: 0.0992\nEpoch 27/60 | Loss: 407.0 | Train: 0.0812 | Val: 0.1107\nEpoch 28/60 | Loss: 398.8 | Train: 0.0930 | Val: 0.1145\n  ↳ Saved!\nEpoch 29/60 | Loss: 399.7 | Train: 0.0908 | Val: 0.1234\n  ↳ Saved!\nEpoch 30/60 | Loss: 393.2 | Train: 0.0939 | Val: 0.1349\n  ↳ Saved!\nEpoch 31/60 | Loss: 392.6 | Train: 0.0889 | Val: 0.1336\nEpoch 32/60 | Loss: 392.4 | Train: 0.0924 | Val: 0.1145\nEpoch 33/60 | Loss: 388.9 | Train: 0.0997 | Val: 0.1158\nEpoch 34/60 | Loss: 390.5 | Train: 0.1102 | Val: 0.1234\nEpoch 35/60 | Loss: 389.2 | Train: 0.1022 | Val: 0.1298\nEpoch 36/60 | Loss: 383.1 | Train: 0.1010 | Val: 0.1221\nEpoch 37/60 | Loss: 374.3 | Train: 0.1150 | Val: 0.1578\n  ↳ Saved!\nEpoch 38/60 | Loss: 368.7 | Train: 0.1191 | Val: 0.1438\nEpoch 39/60 | Loss: 368.7 | Train: 0.1213 | Val: 0.1425\nEpoch 40/60 | Loss: 367.0 | Train: 0.1248 | Val: 0.1399\nEpoch 41/60 | Loss: 362.2 | Train: 0.1239 | Val: 0.1590\n  ↳ Saved!\nEpoch 42/60 | Loss: 362.3 | Train: 0.1274 | Val: 0.1603\n  ↳ Saved!\nEpoch 43/60 | Loss: 357.1 | Train: 0.1389 | Val: 0.1641\n  ↳ Saved!\nEpoch 44/60 | Loss: 356.7 | Train: 0.1261 | Val: 0.1552\nEpoch 45/60 | Loss: 357.8 | Train: 0.1401 | Val: 0.1578\nEpoch 46/60 | Loss: 357.5 | Train: 0.1341 | Val: 0.1616\nEpoch 47/60 | Loss: 351.2 | Train: 0.1522 | Val: 0.1603\nEpoch 48/60 | Loss: 356.7 | Train: 0.1338 | Val: 0.1501\nEpoch 49/60 | Loss: 351.4 | Train: 0.1424 | Val: 0.1399\nEpoch 50/60 | Loss: 348.2 | Train: 0.1567 | Val: 0.1654\n  ↳ Saved!\nEpoch 51/60 | Loss: 346.8 | Train: 0.1459 | Val: 0.1654\nEpoch 52/60 | Loss: 344.3 | Train: 0.1497 | Val: 0.1756\n  ↳ Saved!\nEpoch 53/60 | Loss: 343.2 | Train: 0.1548 | Val: 0.1730\nEpoch 54/60 | Loss: 340.5 | Train: 0.1538 | Val: 0.1743\nEpoch 55/60 | Loss: 338.4 | Train: 0.1599 | Val: 0.1603\nEpoch 56/60 | Loss: 338.6 | Train: 0.1551 | Val: 0.1590\nEpoch 57/60 | Loss: 338.6 | Train: 0.1634 | Val: 0.1590\nEpoch 58/60 | Loss: 337.1 | Train: 0.1726 | Val: 0.1679\nEpoch 59/60 | Loss: 334.8 | Train: 0.1666 | Val: 0.1768\n  ↳ Saved!\nEpoch 60/60 | Loss: 334.4 | Train: 0.1554 | Val: 0.1552\n\nBest: 0.1768\n   id  label\n0   1    117\n1   2     37\n2   3     74\n3   4     88\n4   5     70\nDone!\n","output_type":"stream"}],"execution_count":22}]}