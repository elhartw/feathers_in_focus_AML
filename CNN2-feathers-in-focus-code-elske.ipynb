{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14010757,"sourceType":"datasetVersion","datasetId":8924522}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# **Data preparation**\n\nCreating a dataset that is compatible with torch.DataLoader\n","metadata":{}},{"cell_type":"code","source":"# importing packages\n\nimport numpy as np\nimport pandas as pd\nimport torch as tc\n\nfrom pathlib import Path\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import v2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T09:10:00.687281Z","iopub.execute_input":"2025-12-08T09:10:00.687587Z","iopub.status.idle":"2025-12-08T09:10:00.691850Z","shell.execute_reply.started":"2025-12-08T09:10:00.687572Z","shell.execute_reply":"2025-12-08T09:10:00.691085Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"Transforming images to make them compatible for CNN","metadata":{}},{"cell_type":"code","source":"TRANSFORM_DEFAULT = v2.Compose([\n    v2.ToImage(),\n    v2.Resize((224, 224)),\n    v2.ToDtype(tc.float32, scale=True), \n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T09:10:00.692964Z","iopub.execute_input":"2025-12-08T09:10:00.693169Z","iopub.status.idle":"2025-12-08T09:10:00.710674Z","shell.execute_reply.started":"2025-12-08T09:10:00.693154Z","shell.execute_reply":"2025-12-08T09:10:00.709821Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"DATA_DIR = Path(\"/kaggle/input/feathers-in-focus-model/aml-2025-feathers-in-focus\")\n\nattributes = np.load(DATA_DIR / \"attributes.npy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T09:10:00.711758Z","iopub.execute_input":"2025-12-08T09:10:00.711954Z","iopub.status.idle":"2025-12-08T09:10:00.728967Z","shell.execute_reply.started":"2025-12-08T09:10:00.711938Z","shell.execute_reply":"2025-12-08T09:10:00.728055Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/feathers-in-focus-model//aml-2025-feathers-in-focus/train_images.csv\")\nprint(train_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T09:10:00.730075Z","iopub.execute_input":"2025-12-08T09:10:00.730259Z","iopub.status.idle":"2025-12-08T09:10:00.755722Z","shell.execute_reply.started":"2025-12-08T09:10:00.730240Z","shell.execute_reply":"2025-12-08T09:10:00.754608Z"}},"outputs":[{"name":"stdout","text":"            image_path  label\n0  /train_images/1.jpg      1\n1  /train_images/2.jpg      1\n2  /train_images/3.jpg      1\n3  /train_images/4.jpg      1\n4  /train_images/5.jpg      1\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"Defining the type and config parameters of the data","metadata":{}},{"cell_type":"code","source":"# idx, image, label, path\n\nItemType = tuple[int, tc.Tensor, int, tc.Tensor, str] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T09:10:00.756606Z","iopub.execute_input":"2025-12-08T09:10:00.756853Z","iopub.status.idle":"2025-12-08T09:10:00.770251Z","shell.execute_reply.started":"2025-12-08T09:10:00.756832Z","shell.execute_reply":"2025-12-08T09:10:00.769302Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"Define how to read a sample given the index","metadata":{}},{"cell_type":"code","source":"class ImageClassification(Dataset[ItemType]):\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        search_root: Path,\n        attributes: np.ndarray, \n        transform: v2.Transform = TRANSFORM_DEFAULT,\n    ):\n        self.df = df.reset_index(drop=True)\n        self.search_root = search_root\n        self.transform = transform\n        self.attributes = tc.tensor(attributes, dtype=tc.float32)\n    \n    def __len__(self) -> int:\n        return len(self.df)\n    \n    def _find_image_path(self, filename: str) -> Path:\n        path = self.search_root / filename.lstrip('/')\n        if not path.exists():\n            raise FileNotFoundError(f\"Image file '{filename}' not found at {path}\")\n        return path\n    \n    def __getitem__(self, idx: int) -> ItemType:\n        row = self.df.iloc[idx]\n        filename = Path(str(row[\"image_path\"])).name\n        path = self._find_image_path(filename)\n        \n        image = Image.open(path).convert(\"RGB\")\n        image = self.transform(image)\n        \n        label = int(row[\"label\"]) - 1\n        attr = self.attributes[label]  \n        \n        return idx, image, label, attr, str(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T09:10:00.771607Z","iopub.execute_input":"2025-12-08T09:10:00.771828Z","iopub.status.idle":"2025-12-08T09:10:00.789187Z","shell.execute_reply.started":"2025-12-08T09:10:00.771811Z","shell.execute_reply":"2025-12-08T09:10:00.788515Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"Check if set-up works","metadata":{}},{"cell_type":"code","source":"attributes = np.load(\"/kaggle/input/feathers-in-focus-model/aml-2025-feathers-in-focus/attributes.npy\")\n\nSEARCH_ROOT = Path(\"/kaggle/input/feathers-in-focus-model/aml-2025-feathers-in-focus/train_images\")\n\ntrain_dataset = ImageClassification(\n    df=train_df,\n    search_root=SEARCH_ROOT,\n    attributes=attributes, \n    transform=TRANSFORM_DEFAULT,\n)\n\nidx0, img0, label0, attr0, path0 = train_dataset[0] \nprint(\"Pad eerste image:\", path0)\nprint(\"Image shape:\", img0.shape)\nprint(\"Label:\", label0)\nprint(\"Attributes shape:\", attr0.shape)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T09:10:00.789947Z","iopub.execute_input":"2025-12-08T09:10:00.790228Z","iopub.status.idle":"2025-12-08T09:10:00.818786Z","shell.execute_reply.started":"2025-12-08T09:10:00.790210Z","shell.execute_reply":"2025-12-08T09:10:00.817886Z"}},"outputs":[{"name":"stdout","text":"Pad eerste image: /kaggle/input/feathers-in-focus-model/aml-2025-feathers-in-focus/train_images/1.jpg\nImage shape: torch.Size([3, 224, 224])\nLabel: 0\nAttributes shape: torch.Size([312])\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"Splitting the data in training and validation set (80/20) and creating DataLoader batches","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# Train/validation split\nfrom torch.utils.data import random_split\n\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\ntrain_set, val_set = random_split(train_dataset, [train_size, val_size])\n\n# DataLoaders batches\ntrain_loader = DataLoader(train_set, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T09:10:00.819765Z","iopub.execute_input":"2025-12-08T09:10:00.820043Z","iopub.status.idle":"2025-12-08T09:10:00.841610Z","shell.execute_reply.started":"2025-12-08T09:10:00.820023Z","shell.execute_reply":"2025-12-08T09:10:00.840860Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"# **CREATING, TRAINING, VALIDATING MODEL**","metadata":{}},{"cell_type":"code","source":"import argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom torch.optim.lr_scheduler import StepLR\n\nfrom pathlib import Path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T09:10:00.842181Z","iopub.execute_input":"2025-12-08T09:10:00.842335Z","iopub.status.idle":"2025-12-08T09:10:00.864370Z","shell.execute_reply.started":"2025-12-08T09:10:00.842321Z","shell.execute_reply":"2025-12-08T09:10:00.863421Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"DATA_DIR  = Path(\"/kaggle/input/feathers-in-focus-model/aml-2025-feathers-in-focus\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T09:10:00.890230Z","iopub.execute_input":"2025-12-08T09:10:00.890472Z","iopub.status.idle":"2025-12-08T09:10:00.894400Z","shell.execute_reply.started":"2025-12-08T09:10:00.890458Z","shell.execute_reply":"2025-12-08T09:10:00.893530Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def load_argparser():\n    parser = argparse.ArgumentParser(description=\"Feathers in Focus\")\n    parser.add.argument(\"--batch-size\", type=int, default=64)\n    parser.add.argument(\"__test-batch-size\", type=int, default=1000)\n    parser.add.argument(\"--epochs\", type=int, default=14)\n    parser.add.argument(\"--lr\", type=float, default=1.0)\n    parder.add.argument(\"--gamma\", type=float, default=0.7)\n    parser.add.argument(\"--dry-run\", action=\"store_true\")\n    parser.add.argument(\"--seed\", type=int, default=1)\n    parser.add.argument(\"--log-interval\", type=int, default=10)\n    parder.add.argument(\"--save-model\", action=\"store_true\")\n    return parser","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T09:10:00.895595Z","iopub.execute_input":"2025-12-08T09:10:00.895822Z","iopub.status.idle":"2025-12-08T09:10:00.914367Z","shell.execute_reply.started":"2025-12-08T09:10:00.895808Z","shell.execute_reply":"2025-12-08T09:10:00.913454Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"**Defining the model to use**\n\nModel with 4 convolional layers (RGB, simple features, complex pattorns, higher level features). \nThis leads to a fully convolutional layer with 9216 input and 128 output. \nThis leads to 200 classes, bird species. ","metadata":{}},{"cell_type":"markdown","source":"**Forward pass**\n\nForward pass trough the model, using the 4 convolutional layers, combined with the ReLu activation function for non-linearity. Making use of max pooling and creating the fully connected layer followed by the output layer and softmax function. ","metadata":{}},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n        self.conv4 = nn.Conv2d(64, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 200)     \n        self.fc_attr = nn.Linear(128, 312)   \n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n\n        x = self.conv3(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n\n        x = self.conv4(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        features = self.dropout2(x) \n        \n        class_output = F.log_softmax(self.fc2(features), dim=1)\n        attr_output = torch.sigmoid(self.fc_attr(features))  \n        \n        return class_output, attr_output ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T09:10:00.915408Z","iopub.execute_input":"2025-12-08T09:10:00.915682Z","iopub.status.idle":"2025-12-08T09:10:00.938704Z","shell.execute_reply.started":"2025-12-08T09:10:00.915662Z","shell.execute_reply":"2025-12-08T09:10:00.937683Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"**Training loop: forward - backward propagation**\n\nFrom forward pass, to calculating the loss, to backward propagation calculating the gradient to adjusting the weights. ","metadata":{}},{"cell_type":"code","source":"def train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (idx, data, target, attr_target, path) in enumerate(train_loader):\n        data = data.to(device)\n        target = target.to(device)\n        attr_target = attr_target.to(device)\n        \n        optimizer.zero_grad()\n        class_output, attr_output = model(data) \n        \n        # Gecombineerde loss\n        class_loss = F.nll_loss(class_output, target)\n        attr_loss = F.mse_loss(attr_output, attr_target) \n        loss = class_loss + 0.5 * attr_loss \n        \n        loss.backward()\n        optimizer.step()\n\n        if batch_idx % 10 == 0:\n            print(f\"Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] \"\n                  f\"Loss: {loss.item():.6f} (class: {class_loss.item():.4f}, attr: {attr_loss.item():.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T09:10:00.940433Z","iopub.execute_input":"2025-12-08T09:10:00.940702Z","iopub.status.idle":"2025-12-08T09:10:00.965088Z","shell.execute_reply.started":"2025-12-08T09:10:00.940680Z","shell.execute_reply":"2025-12-08T09:10:00.964168Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"**Testing / validation loop**","metadata":{}},{"cell_type":"code","source":"def test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n\n    with torch.no_grad():\n        for idx, data, target, attr_target, path in test_loader:\n            data = data.to(device)\n            target = target.to(device)\n            \n            class_output, attr_output = model(data) \n            test_loss += F.nll_loss(class_output, target, reduction=\"sum\").item()\n            pred = class_output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    accuracy = 100. * correct / len(test_loader.dataset)\n    print(f\"Val set: Avg loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T09:10:00.966383Z","iopub.execute_input":"2025-12-08T09:10:00.966694Z","iopub.status.idle":"2025-12-08T09:10:00.989374Z","shell.execute_reply.started":"2025-12-08T09:10:00.966675Z","shell.execute_reply":"2025-12-08T09:10:00.988377Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Config\ntorch.manual_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hyperparameters\nEPOCHS = 3\nLR = 1.0\nGAMMA = 0.7\n\n# Model, optimizer, scheduler\nmodel = Net().to(device)\noptimizer = optim.Adadelta(model.parameters(), lr=LR)\nscheduler = StepLR(optimizer, step_size=1, gamma=GAMMA)\n\n# Training loop\nfor epoch in range(1, EPOCHS + 1):\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, val_loader)\n    scheduler.step()\n\n# Model opslaan\ntorch.save(model.state_dict(), \"bird_cnn.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T09:10:00.990091Z","iopub.execute_input":"2025-12-08T09:10:00.990282Z","iopub.status.idle":"2025-12-08T09:19:30.368725Z","shell.execute_reply.started":"2025-12-08T09:10:00.990265Z","shell.execute_reply":"2025-12-08T09:19:30.367626Z"}},"outputs":[{"name":"stdout","text":"Epoch: 1 [0/3140] Loss: 5.421504 (class: 5.3083, attr: 0.2264)\nEpoch: 1 [320/3140] Loss: 5.421217 (class: 5.3081, attr: 0.2263)\nEpoch: 1 [640/3140] Loss: 5.397836 (class: 5.2857, attr: 0.2242)\nEpoch: 1 [960/3140] Loss: 5.460761 (class: 5.3502, attr: 0.2211)\nEpoch: 1 [1280/3140] Loss: 5.389899 (class: 5.2798, attr: 0.2201)\nEpoch: 1 [1600/3140] Loss: 5.389816 (class: 5.2816, attr: 0.2165)\nEpoch: 1 [1920/3140] Loss: 5.405420 (class: 5.2993, attr: 0.2122)\nEpoch: 1 [2240/3140] Loss: 5.374874 (class: 5.2698, attr: 0.2102)\nEpoch: 1 [2560/3140] Loss: 5.357296 (class: 5.2531, attr: 0.2083)\nEpoch: 1 [2880/3140] Loss: 5.372732 (class: 5.2753, attr: 0.1948)\nVal set: Avg loss: 5.2613, Accuracy: 3/786 (0.38%)\nEpoch: 2 [0/3140] Loss: 5.334018 (class: 5.2402, attr: 0.1876)\nEpoch: 2 [320/3140] Loss: 5.388085 (class: 5.2993, attr: 0.1776)\nEpoch: 2 [640/3140] Loss: 5.362556 (class: 5.2753, attr: 0.1745)\nEpoch: 2 [960/3140] Loss: 5.436480 (class: 5.3500, attr: 0.1731)\nEpoch: 2 [1280/3140] Loss: 5.342386 (class: 5.2609, attr: 0.1629)\nEpoch: 2 [1600/3140] Loss: 5.307076 (class: 5.2269, attr: 0.1604)\nEpoch: 2 [1920/3140] Loss: 5.375019 (class: 5.2945, attr: 0.1611)\nEpoch: 2 [2240/3140] Loss: 5.274247 (class: 5.1984, attr: 0.1518)\nEpoch: 2 [2560/3140] Loss: 5.395937 (class: 5.3175, attr: 0.1568)\nEpoch: 2 [2880/3140] Loss: 5.222986 (class: 5.1419, attr: 0.1621)\nVal set: Avg loss: 5.2637, Accuracy: 2/786 (0.25%)\nEpoch: 3 [0/3140] Loss: 5.215508 (class: 5.1467, attr: 0.1376)\nEpoch: 3 [320/3140] Loss: 5.382875 (class: 5.3155, attr: 0.1347)\nEpoch: 3 [640/3140] Loss: 5.417817 (class: 5.3483, attr: 0.1391)\nEpoch: 3 [960/3140] Loss: 5.241183 (class: 5.1765, attr: 0.1293)\nEpoch: 3 [1280/3140] Loss: 5.373429 (class: 5.3008, attr: 0.1452)\nEpoch: 3 [1600/3140] Loss: 5.385389 (class: 5.3169, attr: 0.1369)\nEpoch: 3 [1920/3140] Loss: 5.242488 (class: 5.1780, attr: 0.1289)\nEpoch: 3 [2240/3140] Loss: 5.279292 (class: 5.2186, attr: 0.1214)\nEpoch: 3 [2560/3140] Loss: 5.182628 (class: 5.1265, attr: 0.1123)\nEpoch: 3 [2880/3140] Loss: 5.414609 (class: 5.3555, attr: 0.1181)\nVal set: Avg loss: 5.2636, Accuracy: 4/786 (0.51%)\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"**Creating predictions on the test set and creating a file for submission in Kaggle**","metadata":{}},{"cell_type":"code","source":"class ImageClassification(Dataset):\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        search_root: Path,\n        attributes: np.ndarray = None,  # nu optioneel\n        transform: v2.Transform = TRANSFORM_DEFAULT,\n    ):\n        self.df = df.reset_index(drop=True)\n        self.search_root = search_root\n        self.transform = transform\n        self.attributes = tc.tensor(attributes, dtype=tc.float32) if attributes is not None else None\n\n    def __len__(self) -> int:\n        return len(self.df)\n    \n    def _find_image_path(self, filename: str) -> Path:\n        path = self.search_root / filename.lstrip('/')\n        if not path.exists():\n            raise FileNotFoundError(f\"Image file '{filename}' not found at {path}\")\n        return path\n\n    def __getitem__(self, idx: int):\n        row = self.df.iloc[idx]\n        filename = Path(str(row[\"image_path\"])).name\n        path = self._find_image_path(filename)\n        \n        image = Image.open(path).convert(\"RGB\")\n        image = self.transform(image)\n        \n        if \"label\" in row.index and self.attributes is not None:\n            label = int(row[\"label\"]) - 1\n            attr = self.attributes[label]\n            return idx, image, label, attr, str(path)\n        else:\n            return idx, image, str(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T09:25:12.468654Z","iopub.execute_input":"2025-12-08T09:25:12.468874Z","iopub.status.idle":"2025-12-08T09:25:12.476592Z","shell.execute_reply.started":"2025-12-08T09:25:12.468860Z","shell.execute_reply":"2025-12-08T09:25:12.475648Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/feathers-in-focus-model/aml-2025-feathers-in-focus/test_images_path.csv\")\ntest_dataset = ImageClassification(\n    df=test_df,\n    search_root=Path(\"/kaggle/input/feathers-in-focus-model/aml-2025-feathers-in-focus/test_images\"),\n    transform=TRANSFORM_DEFAULT,\n)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nmodel.eval()\npredictions = []\n\nwith torch.no_grad():\n    for idx, data, path in test_loader:\n        data = data.to(device)\n        class_output, attr_output = model(data)\n        pred = class_output.argmax(dim=1) + 1\n        predictions.extend(pred.tolist())\n\nsubmission = pd.DataFrame({\n    \"id\": test_df[\"id\"].values,\n    \"label\": predictions\n})\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(f\"Aantal predictions: {len(predictions)}\")\nprint(f\"Aantal test samples: {len(test_df)}\")\nprint(submission.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-08T09:35:16.237069Z","iopub.execute_input":"2025-12-08T09:35:16.237854Z","iopub.status.idle":"2025-12-08T09:37:06.742813Z","shell.execute_reply.started":"2025-12-08T09:35:16.237819Z","shell.execute_reply":"2025-12-08T09:37:06.741537Z"}},"outputs":[{"name":"stdout","text":"Aantal predictions: 4000\nAantal test samples: 4000\n   id  label\n0   1      4\n1   2      4\n2   3      4\n3   4      4\n4   5      4\n5   6      4\n6   7      4\n7   8      4\n8   9      4\n9  10      4\n","output_type":"stream"}],"execution_count":54}]}